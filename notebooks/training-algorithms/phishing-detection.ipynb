{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:24.970633Z",
     "iopub.status.busy": "2023-04-13T20:50:24.969726Z",
     "iopub.status.idle": "2023-04-13T20:50:27.661961Z",
     "shell.execute_reply": "2023-04-13T20:50:27.661389Z",
     "shell.execute_reply.started": "2023-04-13T20:11:45.718621Z"
    },
    "id": "cj9NXWy3zd4T",
    "outputId": "0d72ee04-eea7-42f6-e1f0-d7fccc461a9a",
    "papermill": {
     "duration": 2.712362,
     "end_time": "2023-04-13T20:50:27.662067",
     "exception": false,
     "start_time": "2023-04-13T20:50:24.949705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu up\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 32\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import  f1_score\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.nn  import functional as F\n",
    "import torch.optim as  optim \n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  print(\"gpu up\")\n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:27.703259Z",
     "iopub.status.busy": "2023-04-13T20:50:27.699692Z",
     "iopub.status.idle": "2023-04-13T20:50:29.039317Z",
     "shell.execute_reply": "2023-04-13T20:50:29.037937Z",
     "shell.execute_reply.started": "2023-04-13T20:11:49.075931Z"
    },
    "id": "k5O16FLBzhuC",
    "outputId": "6f53909e-a92b-46c6-ed25-95a67abfc8ca",
    "papermill": {
     "duration": 1.361612,
     "end_time": "2023-04-13T20:50:29.039470",
     "exception": false,
     "start_time": "2023-04-13T20:50:27.677858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "those are the libraries I use for processing text\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem  import PorterStemmer\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def removepunc(my_str): # function to remove punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "snowstem = SnowballStemmer(\"english\")\n",
    "portstem = PorterStemmer()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:29.106507Z",
     "iopub.status.busy": "2023-04-13T20:50:29.105849Z",
     "iopub.status.idle": "2023-04-13T20:50:29.991186Z",
     "shell.execute_reply": "2023-04-13T20:50:29.990657Z",
     "shell.execute_reply.started": "2023-04-13T20:11:50.617565Z"
    },
    "id": "elXvdWQyzkDw",
    "papermill": {
     "duration": 0.92553,
     "end_time": "2023-04-13T20:50:29.991315",
     "exception": false,
     "start_time": "2023-04-13T20:50:29.065785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"/kaggle/input/phishing-detection/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/phishing-detection/test.csv\")\n",
    "traindata.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:30.033430Z",
     "iopub.status.busy": "2023-04-13T20:50:30.032656Z",
     "iopub.status.idle": "2023-04-13T20:50:30.040735Z",
     "shell.execute_reply": "2023-04-13T20:50:30.041140Z",
     "shell.execute_reply.started": "2023-04-13T20:11:51.678805Z"
    },
    "papermill": {
     "duration": 0.033718,
     "end_time": "2023-04-13T20:50:30.041291",
     "exception": false,
     "start_time": "2023-04-13T20:50:30.007573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xml.coverpages.org/xmlForms.html</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pawsoft.com/files/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ibegin.com/directory/ca/quebec/anjou/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mxp4016.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hooksgems.blogspot.com/2009/09/clark-terry-cla...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368056</th>\n",
       "      <td>pt-tkbi.com/providernet/provider/provider/webm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368057</th>\n",
       "      <td>wiki.d-addicts.com/Ninomiya_Kazunari</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368058</th>\n",
       "      <td>jlkc.org/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368059</th>\n",
       "      <td>picobong.com/www.redirect.com.htm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368060</th>\n",
       "      <td>hacticdocs.org/sz/fox/dropbox</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368061 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label\n",
       "0                        xml.coverpages.org/xmlForms.html      2\n",
       "1                                      pawsoft.com/files/      1\n",
       "2                   ibegin.com/directory/ca/quebec/anjou/      2\n",
       "3                                             mxp4016.com      1\n",
       "4       hooksgems.blogspot.com/2009/09/clark-terry-cla...      2\n",
       "...                                                   ...    ...\n",
       "368056  pt-tkbi.com/providernet/provider/provider/webm...      1\n",
       "368057               wiki.d-addicts.com/Ninomiya_Kazunari      2\n",
       "368058                                          jlkc.org/      2\n",
       "368059                  picobong.com/www.redirect.com.htm      1\n",
       "368060                      hacticdocs.org/sz/fox/dropbox      1\n",
       "\n",
       "[368061 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:30.076825Z",
     "iopub.status.busy": "2023-04-13T20:50:30.075973Z",
     "iopub.status.idle": "2023-04-13T20:50:30.078843Z",
     "shell.execute_reply": "2023-04-13T20:50:30.078438Z",
     "shell.execute_reply.started": "2023-04-13T20:11:51.699772Z"
    },
    "id": "lTDXODNhzwwz",
    "papermill": {
     "duration": 0.022821,
     "end_time": "2023-04-13T20:50:30.078939",
     "exception": false,
     "start_time": "2023-04-13T20:50:30.056118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function is the tokenizer we are using, it does basic processing also  like ,\n",
    "Lowercase the text\n",
    "removing punctuation, stop words and numbers,\n",
    "it also removes extra spaces and unwanted characters (I use regex for that)\n",
    "\n",
    "\n",
    "before using the tokenizer I was testing it on the train dataframe manually  \n",
    "\"\"\"\n",
    "\n",
    "def myTokenizer(x):\n",
    " return  [snowstem.stem(word.text)for word in \n",
    "          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n",
    "          if (word.text not in stops and not hasNumbers(word.text)) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:50:30.117577Z",
     "iopub.status.busy": "2023-04-13T20:50:30.116680Z",
     "iopub.status.idle": "2023-04-13T20:54:19.185463Z",
     "shell.execute_reply": "2023-04-13T20:54:19.184883Z",
     "shell.execute_reply.started": "2023-04-13T20:11:51.708149Z"
    },
    "id": "udmV7yOmPNt6",
    "papermill": {
     "duration": 229.091672,
     "end_time": "2023-04-13T20:54:19.185582",
     "exception": false,
     "start_time": "2023-04-13T20:50:30.093910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here I'm using the torchtext fields and dataset classes they can ease the work to get\n",
    "the dataset ready for the pytorch model\n",
    "\n",
    "the class DataFrameDataset is the easiest way I found to turn a dataframe into a torchtext dataset\n",
    "\n",
    "this cell will take sometime to finish\n",
    "\"\"\"\n",
    "\n",
    "TEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
    "LABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n",
    "\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('comment_text', text_field), ('toxic', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.Label \n",
    "            text = row.URL\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "  \n",
    "\n",
    "torchdataset = DataFrameDataset(traindata, TEXT,LABEL)\n",
    "torchtest = DataFrameDataset(test, TEXT,LABEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:19.257390Z",
     "iopub.status.busy": "2023-04-13T20:54:19.242041Z",
     "iopub.status.idle": "2023-04-13T20:54:19.706607Z",
     "shell.execute_reply": "2023-04-13T20:54:19.705622Z",
     "shell.execute_reply.started": "2023-04-13T20:15:53.855851Z"
    },
    "id": "-f1hgGywizQT",
    "papermill": {
     "duration": 0.505413,
     "end_time": "2023-04-13T20:54:19.706733",
     "exception": false,
     "start_time": "2023-04-13T20:54:19.201320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:19.801615Z",
     "iopub.status.busy": "2023-04-13T20:54:19.765711Z",
     "iopub.status.idle": "2023-04-13T20:54:21.744862Z",
     "shell.execute_reply": "2023-04-13T20:54:21.744359Z",
     "shell.execute_reply.started": "2023-04-13T20:15:54.344750Z"
    },
    "id": "O6PUzivRJvL_",
    "papermill": {
     "duration": 2.022837,
     "end_time": "2023-04-13T20:54:21.744978",
     "exception": false,
     "start_time": "2023-04-13T20:54:19.722141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this cell build the vocab which means it get all the used words and if also ignores any word \n",
    "that only appeared less than 3 times\n",
    "\"\"\"\n",
    "TEXT.build_vocab(train_data,min_freq=3)  \n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:21.804763Z",
     "iopub.status.busy": "2023-04-13T20:54:21.794601Z",
     "iopub.status.idle": "2023-04-13T20:54:21.830876Z",
     "shell.execute_reply": "2023-04-13T20:54:21.830381Z",
     "shell.execute_reply.started": "2023-04-13T20:15:56.240738Z"
    },
    "id": "QojEJaoBVTJj",
    "outputId": "528174ce-a162-47bf-edb4-ab3358bf296a",
    "papermill": {
     "duration": 0.070821,
     "end_time": "2023-04-13T20:54:21.830977",
     "exception": false,
     "start_time": "2023-04-13T20:54:21.760156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 36857\n",
      "Size of LABEL vocabulary: 2\n",
      "[('com', 210421), ('html', 43390), ('www', 28719), ('org', 26847), ('net', 18407), ('amp', 17995), ('htm', 17088), ('php', 16645), ('en', 16334), ('index', 15289)]\n"
     ]
    }
   ],
   "source": [
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:21.865352Z",
     "iopub.status.busy": "2023-04-13T20:54:21.864759Z",
     "iopub.status.idle": "2023-04-13T20:54:21.868595Z",
     "shell.execute_reply": "2023-04-13T20:54:21.868126Z",
     "shell.execute_reply.started": "2023-04-13T20:15:56.298834Z"
    },
    "id": "JVrwFrmTqHzc",
    "papermill": {
     "duration": 0.022872,
     "end_time": "2023-04-13T20:54:21.868684",
     "exception": false,
     "start_time": "2023-04-13T20:54:21.845812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\"\"\"\n",
    "we are using batches for validation and test set because of memory usage we can't pass the whole set at once \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
    "    (train_data,valid_data,torchtest), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort =False,\n",
    "shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:21.909154Z",
     "iopub.status.busy": "2023-04-13T20:54:21.908574Z",
     "iopub.status.idle": "2023-04-13T20:54:26.797660Z",
     "shell.execute_reply": "2023-04-13T20:54:26.797208Z",
     "shell.execute_reply.started": "2023-04-13T20:15:56.310275Z"
    },
    "id": "QA57LLmjMCX3",
    "outputId": "964eb82a-0855-4675-a5c7-7ae4f6859d1e",
    "papermill": {
     "duration": 4.914202,
     "end_time": "2023-04-13T20:54:26.797770",
     "exception": false,
     "start_time": "2023-04-13T20:54:21.883568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextTransformer(\n",
       "  (wordEmbeddings): Embedding(36857, 140)\n",
       "  (positionEmbeddings): Embedding(140, 20)\n",
       "  (transformerLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=160, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=160, bias=True)\n",
       "    (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=160, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (linear3): Linear(in_features=140, out_features=16, bias=True)\n",
       "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "one major point here is that I encoded the embeddings in a different way \n",
    "I made an embedding layer for the position then I concatenated position embeddings with the word embeddings \n",
    "just thought it could be a usefull way to encode the positions \n",
    "\n",
    "had to reshape the output of the transformer layer to get the prediction\n",
    "\"\"\"\n",
    "class TextTransformer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextTransformer,self).__init__()\n",
    "    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n",
    "    self.positionEmbeddings = nn.Embedding(140,20)\n",
    "    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n",
    "    self.linear1 = nn.Linear(160,  64)\n",
    "    self.linear2 = nn.Linear(64,  1)\n",
    "    self.linear3 = nn.Linear(140,  16)\n",
    "    self.linear4 = nn.Linear(16,  1)\n",
    "  def forward(self,x):\n",
    "    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n",
    "    # broadcasting the tensor of positions \n",
    "    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n",
    "    attended = self.transformerLayer(sentence)\n",
    "    linear1 = F.relu(self.linear1(attended))\n",
    "    linear2 = F.relu(self.linear2(linear1))\n",
    "    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
    "    linear3 = F.relu(self.linear3(linear2))\n",
    "    out = torch.sigmoid(self.linear4(linear3))\n",
    "    return out\n",
    "\n",
    "myTransformer = TextTransformer()\n",
    "myTransformer.to(device)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:26.835466Z",
     "iopub.status.busy": "2023-04-13T20:54:26.834806Z",
     "iopub.status.idle": "2023-04-13T20:54:26.837757Z",
     "shell.execute_reply": "2023-04-13T20:54:26.837334Z",
     "shell.execute_reply.started": "2023-04-13T20:16:01.599306Z"
    },
    "id": "OQOBpRBUAyAk",
    "papermill": {
     "duration": 0.024182,
     "end_time": "2023-04-13T20:54:26.837853",
     "exception": false,
     "start_time": "2023-04-13T20:54:26.813671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculateMetrics(ypred,ytrue):\n",
    "  acc  = accuracy_score(ytrue,ypred)\n",
    "  f1  = f1_score(ytrue,ypred)\n",
    "  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n",
    "  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T20:54:26.880243Z",
     "iopub.status.busy": "2023-04-13T20:54:26.879628Z",
     "iopub.status.idle": "2023-04-13T21:23:01.777011Z",
     "shell.execute_reply": "2023-04-13T21:23:01.776531Z",
     "shell.execute_reply.started": "2023-04-13T20:16:01.609844Z"
    },
    "id": "zbLZTzgxJRIA",
    "outputId": "21fbc41e-a284-4267-d3ca-0c10387db2a7",
    "papermill": {
     "duration": 1714.923817,
     "end_time": "2023-04-13T21:23:01.777144",
     "exception": false,
     "start_time": "2023-04-13T20:54:26.853327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BCE loss:  0.4033504128456116  f1 score: 0.587 f1 average: 0.733 accuracy: 0.814\n",
      "validation BCE loss:  0.300700306892395  f1 score: 0.726 f1 average: 0.821 accuracy: 0.871\n",
      "train BCE loss:  0.2577840983867645  f1 score: 0.794 f1 average: 0.86 accuracy: 0.892\n",
      "validation BCE loss:  0.2441278100013733  f1 score: 0.796 f1 average: 0.864 accuracy: 0.898\n",
      "train BCE loss:  0.22502966225147247  f1 score: 0.822 f1 average: 0.879 accuracy: 0.905\n",
      "validation BCE loss:  0.2225685864686966  f1 score: 0.817 f1 average: 0.877 accuracy: 0.906\n",
      "train BCE loss:  0.20954887568950653  f1 score: 0.838 f1 average: 0.889 accuracy: 0.913\n",
      "validation BCE loss:  0.21045918762683868  f1 score: 0.828 f1 average: 0.884 accuracy: 0.91\n",
      "train BCE loss:  0.20062369108200073  f1 score: 0.846 f1 average: 0.894 accuracy: 0.917\n",
      "validation BCE loss:  0.20341674983501434  f1 score: 0.838 f1 average: 0.89 accuracy: 0.914\n",
      "train BCE loss:  0.1946922242641449  f1 score: 0.851 f1 average: 0.898 accuracy: 0.919\n",
      "validation BCE loss:  0.19960756599903107  f1 score: 0.839 f1 average: 0.89 accuracy: 0.915\n",
      "train BCE loss:  0.1899084746837616  f1 score: 0.856 f1 average: 0.901 accuracy: 0.921\n",
      "validation BCE loss:  0.1964939534664154  f1 score: 0.844 f1 average: 0.893 accuracy: 0.917\n",
      "train BCE loss:  0.18589624762535095  f1 score: 0.859 f1 average: 0.903 accuracy: 0.923\n",
      "validation BCE loss:  0.19349296391010284  f1 score: 0.848 f1 average: 0.896 accuracy: 0.919\n",
      "train BCE loss:  0.18245209753513336  f1 score: 0.863 f1 average: 0.905 accuracy: 0.925\n",
      "validation BCE loss:  0.1905982792377472  f1 score: 0.853 f1 average: 0.899 accuracy: 0.921\n",
      "train BCE loss:  0.17978082597255707  f1 score: 0.865 f1 average: 0.907 accuracy: 0.926\n",
      "validation BCE loss:  0.18799777328968048  f1 score: 0.855 f1 average: 0.9 accuracy: 0.921\n",
      "train BCE loss:  0.17744305729866028  f1 score: 0.867 f1 average: 0.908 accuracy: 0.927\n",
      "validation BCE loss:  0.18644942343235016  f1 score: 0.855 f1 average: 0.901 accuracy: 0.922\n",
      "train BCE loss:  0.17519401013851166  f1 score: 0.87 f1 average: 0.91 accuracy: 0.928\n",
      "validation BCE loss:  0.18607394397258759  f1 score: 0.856 f1 average: 0.901 accuracy: 0.922\n",
      "train BCE loss:  0.17270243167877197  f1 score: 0.872 f1 average: 0.912 accuracy: 0.93\n",
      "validation BCE loss:  0.1836748719215393  f1 score: 0.858 f1 average: 0.903 accuracy: 0.924\n",
      "train BCE loss:  0.17112527787685394  f1 score: 0.873 f1 average: 0.912 accuracy: 0.93\n",
      "validation BCE loss:  0.1829947680234909  f1 score: 0.859 f1 average: 0.903 accuracy: 0.924\n",
      "train BCE loss:  0.16932609677314758  f1 score: 0.875 f1 average: 0.914 accuracy: 0.931\n",
      "validation BCE loss:  0.1807081252336502  f1 score: 0.861 f1 average: 0.904 accuracy: 0.924\n",
      "train BCE loss:  0.167533740401268  f1 score: 0.878 f1 average: 0.916 accuracy: 0.933\n",
      "validation BCE loss:  0.18041503429412842  f1 score: 0.862 f1 average: 0.905 accuracy: 0.925\n",
      "train BCE loss:  0.16605985164642334  f1 score: 0.879 f1 average: 0.916 accuracy: 0.933\n",
      "validation BCE loss:  0.17889191210269928  f1 score: 0.864 f1 average: 0.906 accuracy: 0.926\n",
      "train BCE loss:  0.16474591195583344  f1 score: 0.879 f1 average: 0.917 accuracy: 0.934\n",
      "validation BCE loss:  0.17729398608207703  f1 score: 0.867 f1 average: 0.909 accuracy: 0.928\n",
      "train BCE loss:  0.16259224712848663  f1 score: 0.881 f1 average: 0.918 accuracy: 0.934\n",
      "validation BCE loss:  0.17607587575912476  f1 score: 0.867 f1 average: 0.909 accuracy: 0.928\n",
      "train BCE loss:  0.16126500070095062  f1 score: 0.882 f1 average: 0.918 accuracy: 0.935\n",
      "validation BCE loss:  0.17408733069896698  f1 score: 0.869 f1 average: 0.91 accuracy: 0.929\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "using adagrad because it assign bigger updates to less frequently updated weights \n",
    "(like words that are not used many times)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "optimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)\n",
    "\n",
    "for i in range(20):\n",
    "  trainpreds = torch.tensor([])\n",
    "  traintrues = torch.tensor([])\n",
    "  for  batch in train_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    myTransformer.zero_grad()\n",
    "    pred = myTransformer(X).squeeze()\n",
    "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
    "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
    "    err = F.binary_cross_entropy(pred,y)\n",
    "    err.backward()\n",
    "    optimizer.step()\n",
    "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
    "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
    " \n",
    "\n",
    "  valpreds = torch.tensor([])\n",
    "  valtrues = torch.tensor([])\n",
    "  for batch in valid_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    # print(valtrues.shape)\n",
    "    valpreds = torch.cat((valpreds,pred))\n",
    "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
    "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T21:23:01.975497Z",
     "iopub.status.busy": "2023-04-13T21:23:01.974850Z",
     "iopub.status.idle": "2023-04-13T21:23:02.027831Z",
     "shell.execute_reply": "2023-04-13T21:23:02.027421Z",
     "shell.execute_reply.started": "2023-04-13T20:46:22.885735Z"
    },
    "papermill": {
     "duration": 0.081688,
     "end_time": "2023-04-13T21:23:02.027924",
     "exception": false,
     "start_time": "2023-04-13T21:23:01.946236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TextTransformer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(myTransformer.state_dict(),\"Model\")\n",
    "torch.save(myTransformer.state_dict, \"Model.pt\")\n",
    "torch.save(myTransformer.state_dict, \"Model.pickle\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T21:23:02.300370Z",
     "iopub.status.busy": "2023-04-13T21:23:02.299587Z",
     "iopub.status.idle": "2023-04-13T21:23:22.002883Z",
     "shell.execute_reply": "2023-04-13T21:23:22.002216Z",
     "shell.execute_reply.started": "2023-04-13T20:44:55.648710Z"
    },
    "id": "vordglMh4GGC",
    "outputId": "b65b56fa-c86c-449d-8c97-83e479f537fa",
    "papermill": {
     "duration": 19.749901,
     "end_time": "2023-04-13T21:23:22.002989",
     "exception": false,
     "start_time": "2023-04-13T21:23:02.253088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test BCE loss:  0.17359605431556702  f1 score: 0.867 f1 average: 0.909 accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "now getting the results on the test set\n",
    "\"\"\"\n",
    "\n",
    "testpreds = torch.tensor([])\n",
    "testtrues = torch.tensor([])\n",
    "for batch in test_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    testtrues = torch.cat((testtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    # print(valtrues.shape)\n",
    "    testpreds = torch.cat((testpreds,pred))\n",
    "err = F.binary_cross_entropy(testpreds,testtrues)\n",
    "print(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T21:23:22.060223Z",
     "iopub.status.busy": "2023-04-13T21:23:22.059413Z",
     "iopub.status.idle": "2023-04-13T21:23:22.074289Z",
     "shell.execute_reply": "2023-04-13T21:23:22.074711Z",
     "shell.execute_reply.started": "2023-04-13T20:45:15.692562Z"
    },
    "papermill": {
     "duration": 0.045628,
     "end_time": "2023-04-13T21:23:22.074848",
     "exception": false,
     "start_time": "2023-04-13T21:23:22.029220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mygreenstone.ca/security/secure-code7/security...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>brynncampbell.com/cache/domain/secure-domains/...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>syamasahithi.com/8fh34f3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>www.ppok.asia/login/en/index.htm</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>platinumwindowcleaning.com/wp-content/plugins/...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  Label  predicted\n",
       "108  mygreenstone.ca/security/secure-code7/security...      1        1.0\n",
       "114  brynncampbell.com/cache/domain/secure-domains/...      1        1.0\n",
       "118                           syamasahithi.com/8fh34f3      1        1.0\n",
       "121                   www.ppok.asia/login/en/index.htm      1        1.0\n",
       "128  platinumwindowcleaning.com/wp-content/plugins/...      1        1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"predicted\"] = torch.round(testpreds).numpy()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this shows that the model understands the language well \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test[test.predicted==1].iloc[32:37]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "duration": 1985.545745,
   "end_time": "2023-04-13T21:23:22.414544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-13T20:50:16.868799",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
