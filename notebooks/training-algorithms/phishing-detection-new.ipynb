{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"as this will be a classification task, we will just need the encoder part of the transformer, so I used the transformer encoder layer from pytorch","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import random\n\nSEED = 32\nrandom.seed(SEED)\n\nimport numpy as np \nimport pandas as pd\nimport spacy\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import  f1_score\n\nfrom torch import nn\nimport torch\nfrom torchtext import data\nfrom torch.nn  import functional as F\nimport torch.optim as  optim \n\n\"\"\"\n\nthose are the libraries I use for processing text\n\n\"\"\"\n\nimport nltk\nnltk.download(\"punkt\")\n\nimport re\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nnlp = English()\n\ntokenizer = Tokenizer(nlp.vocab)\n\nfrom nltk import word_tokenize,sent_tokenize\nfrom nltk.stem  import PorterStemmer\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\nstops = stopwords.words(\"english\")\n","metadata":{"id":"cj9NXWy3zd4T","outputId":"0d72ee04-eea7-42f6-e1f0-d7fccc461a9a","execution":{"iopub.status.busy":"2023-04-13T22:27:54.752670Z","iopub.execute_input":"2023-04-13T22:27:54.753051Z","iopub.status.idle":"2023-04-13T22:27:59.206679Z","shell.execute_reply.started":"2023-04-13T22:27:54.753017Z","shell.execute_reply":"2023-04-13T22:27:59.205764Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Activate GPU","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():  \n  dev = \"cuda:0\" \n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T22:27:59.208428Z","iopub.execute_input":"2023-04-13T22:27:59.208989Z","iopub.status.idle":"2023-04-13T22:27:59.283946Z","shell.execute_reply.started":"2023-04-13T22:27:59.208949Z","shell.execute_reply":"2023-04-13T22:27:59.281718Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"gpu up\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Defining functions for data preprocessing","metadata":{}},{"cell_type":"code","source":"def removepunc(my_str): # function to remove punctuation\n    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    return no_punct\n\ndef hasNumbers(inputString):\n    return bool(re.search(r'\\d', inputString))\nsnowstem = SnowballStemmer(\"english\")\nportstem = PorterStemmer()\n\n\n\ndef myTokenizer(x):\n    \"\"\"\n    this function is the tokenizer we are using, it does basic processing also  like ,\n    Lowercase the text\n    removing punctuation, stop words and numbers,\n    it also removes extra spaces and unwanted characters (I use regex for that)\n\n\n    before using the tokenizer I was testing it on the train dataframe manually  \n    \"\"\"\n    return  [snowstem.stem(word.text)for word in \n              tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n              if (word.text not in stops and not hasNumbers(word.text)) ]\n\n","metadata":{"id":"k5O16FLBzhuC","outputId":"6f53909e-a92b-46c6-ed25-95a67abfc8ca","execution":{"iopub.status.busy":"2023-04-13T22:27:59.285154Z","iopub.execute_input":"2023-04-13T22:27:59.285484Z","iopub.status.idle":"2023-04-13T22:27:59.366322Z","shell.execute_reply.started":"2023-04-13T22:27:59.285456Z","shell.execute_reply":"2023-04-13T22:27:59.365629Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"code","source":"traindata = pd.read_csv(\"/kaggle/input/phishing-detection/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/phishing-detection/test.csv\")\ntraindata.drop(\"Unnamed: 0\",axis=1,inplace=True)\ntest.drop(\"Unnamed: 0\",axis=1,inplace=True)","metadata":{"id":"elXvdWQyzkDw","execution":{"iopub.status.busy":"2023-04-13T22:28:02.324655Z","iopub.execute_input":"2023-04-13T22:28:02.324993Z","iopub.status.idle":"2023-04-13T22:28:03.446872Z","shell.execute_reply.started":"2023-04-13T22:28:02.324962Z","shell.execute_reply":"2023-04-13T22:28:03.446032Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"traindata","metadata":{"execution":{"iopub.status.busy":"2023-04-13T22:28:03.448429Z","iopub.execute_input":"2023-04-13T22:28:03.448817Z","iopub.status.idle":"2023-04-13T22:28:03.468850Z","shell.execute_reply.started":"2023-04-13T22:28:03.448778Z","shell.execute_reply":"2023-04-13T22:28:03.467828Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                      URL  Label\n0                        xml.coverpages.org/xmlForms.html      2\n1                                      pawsoft.com/files/      1\n2                   ibegin.com/directory/ca/quebec/anjou/      2\n3                                             mxp4016.com      1\n4       hooksgems.blogspot.com/2009/09/clark-terry-cla...      2\n...                                                   ...    ...\n368056  pt-tkbi.com/providernet/provider/provider/webm...      1\n368057               wiki.d-addicts.com/Ninomiya_Kazunari      2\n368058                                          jlkc.org/      2\n368059                  picobong.com/www.redirect.com.htm      1\n368060                      hacticdocs.org/sz/fox/dropbox      1\n\n[368061 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URL</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xml.coverpages.org/xmlForms.html</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pawsoft.com/files/</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ibegin.com/directory/ca/quebec/anjou/</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mxp4016.com</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hooksgems.blogspot.com/2009/09/clark-terry-cla...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>368056</th>\n      <td>pt-tkbi.com/providernet/provider/provider/webm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>368057</th>\n      <td>wiki.d-addicts.com/Ninomiya_Kazunari</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>368058</th>\n      <td>jlkc.org/</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>368059</th>\n      <td>picobong.com/www.redirect.com.htm</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>368060</th>\n      <td>hacticdocs.org/sz/fox/dropbox</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>368061 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pytorch torchtext\n\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\nhere I'm using the torchtext fields and dataset classes they can ease the work to get\nthe dataset ready for the pytorch model\n\nthe class DataFrameDataset is the easiest way I found to turn a dataframe into a torchtext dataset\n\nthis cell will take sometime to finish\n\"\"\"\n\nTEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\nLABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n\n\nclass DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n        fields = [('url', text_field), ('label', label_field)]\n        examples = []\n        for i, row in df.iterrows():\n            label = row.Label \n            text = row.URL\n            examples.append(data.Example.fromlist([text, label], fields))\n        super().__init__(examples, fields, **kwargs)\n  \n\ntorchdataset = DataFrameDataset(traindata[1:5], TEXT,LABEL)\ntorchtest = DataFrameDataset(test[1:5], TEXT,LABEL)","metadata":{"id":"udmV7yOmPNt6","execution":{"iopub.status.busy":"2023-04-13T22:29:33.973388Z","iopub.execute_input":"2023-04-13T22:29:33.973730Z","iopub.status.idle":"2023-04-13T22:29:33.992153Z","shell.execute_reply.started":"2023-04-13T22:29:33.973700Z","shell.execute_reply":"2023-04-13T22:29:33.991233Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))","metadata":{"id":"-f1hgGywizQT","execution":{"iopub.status.busy":"2023-04-13T22:30:39.291076Z","iopub.execute_input":"2023-04-13T22:30:39.291408Z","iopub.status.idle":"2023-04-13T22:30:39.295800Z","shell.execute_reply.started":"2023-04-13T22:30:39.291377Z","shell.execute_reply":"2023-04-13T22:30:39.294924Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nthis cell build the vocab which means it get all the used words and if also ignores any word \nthat only appeared less than 3 times\n\"\"\"\nTEXT.build_vocab(train_data,min_freq=3)  \nLABEL.build_vocab(train_data)\n","metadata":{"id":"O6PUzivRJvL_","execution":{"iopub.status.busy":"2023-04-13T22:30:40.087333Z","iopub.execute_input":"2023-04-13T22:30:40.087664Z","iopub.status.idle":"2023-04-13T22:30:40.092148Z","shell.execute_reply.started":"2023-04-13T22:30:40.087632Z","shell.execute_reply":"2023-04-13T22:30:40.091308Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10))  \n","metadata":{"id":"QojEJaoBVTJj","outputId":"528174ce-a162-47bf-edb4-ab3358bf296a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set batch size\nBATCH_SIZE = 128\n\n\"\"\"\nwe are using batches for validation and test set because of memory usage we can't pass the whole set at once \n\"\"\"\n\n\ntrain_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n    (train_data,valid_data,torchtest), \n    batch_size = BATCH_SIZE,\n    device = device,\n    sort =False,\nshuffle=False)\n","metadata":{"id":"JVrwFrmTqHzc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"\none major point here is that I encoded the embeddings in a different way \nI made an embedding layer for the position then I concatenated position embeddings with the word embeddings \njust thought it could be a usefull way to encode the positions \n\nhad to reshape the output of the transformer layer to get the prediction\n\"\"\"\nclass TextTransformer(nn.Module):\n  def __init__(self):\n    super(TextTransformer,self).__init__()\n    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n    self.positionEmbeddings = nn.Embedding(140,20)\n    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n    self.linear1 = nn.Linear(160,  64)\n    self.linear2 = nn.Linear(64,  1)\n    self.linear3 = nn.Linear(140,  16)\n    self.linear4 = nn.Linear(16,  1)\n  def forward(self,x):\n    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n    # broadcasting the tensor of positions \n    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n    attended = self.transformerLayer(sentence)\n    linear1 = F.relu(self.linear1(attended))\n    linear2 = F.relu(self.linear2(linear1))\n    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n    linear3 = F.relu(self.linear3(linear2))\n    out = torch.sigmoid(self.linear4(linear3))\n    return out\n\nmyTransformer = TextTransformer()\nmyTransformer.to(device)\n\n    \n","metadata":{"id":"QA57LLmjMCX3","outputId":"964eb82a-0855-4675-a5c7-7ae4f6859d1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculateMetrics(ypred,ytrue):\n  acc  = accuracy_score(ytrue,ypred)\n  f1  = f1_score(ytrue,ypred)\n  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))\n  ","metadata":{"id":"OQOBpRBUAyAk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nusing adagrad because it assign bigger updates to less frequently updated weights \n(like words that are not used many times)\n\n\"\"\"\n\noptimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)\n\nfor i in range(2):\n  trainpreds = torch.tensor([])\n  traintrues = torch.tensor([])\n  for  batch in train_iterator:\n    X = batch.comment_text\n    y = batch.toxic\n    myTransformer.zero_grad()\n    pred = myTransformer(X).squeeze()\n    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n    traintrues = torch.cat((traintrues,y.cpu().detach()))\n    err = F.binary_cross_entropy(pred,y)\n    err.backward()\n    optimizer.step()\n  err = F.binary_cross_entropy(trainpreds,traintrues)\n  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n \n\n  valpreds = torch.tensor([])\n  valtrues = torch.tensor([])\n  for batch in valid_iterator:\n    X = batch.comment_text\n    y = batch.toxic\n    valtrues = torch.cat((valtrues,y.cpu().detach()))\n    pred = myTransformer(X).squeeze().cpu().detach()\n    # print(valtrues.shape)\n    valpreds = torch.cat((valpreds,pred))\n  err = F.binary_cross_entropy(valpreds,valtrues)\n  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n  ","metadata":{"id":"zbLZTzgxJRIA","outputId":"21fbc41e-a284-4267-d3ca-0c10387db2a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so the final scores on validation are  \n\nvalidation BCE loss:  0.137 f1 score: 0.706 f1 average: 0.84 accuracy: 0.952","metadata":{}},{"cell_type":"code","source":"torch.save(myTransformer.state_dict(),\"Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(myTransformer, 'model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(myTransformer.state_dict, \"Model.pt\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(myTransformer.state_dict, \"Model.pickle\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nnow getting the results on the test set\n\"\"\"\n\ntestpreds = torch.tensor([])\ntesttrues = torch.tensor([])\nfor batch in \"syamasahithi.com/8fh34f3\":\n    X = batch.comment_text\n    y = batch.toxic\n    testtrues = torch.cat((testtrues,y.cpu().detach()))\n    pred = myTransformer(X).squeeze().cpu().detach()\n    # print(valtrues.shape)\n    testpreds = torch.cat((testpreds,pred))\nerr = F.binary_cross_entropy(testpreds,testtrues)\nprint(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))\n  ","metadata":{"id":"vordglMh4GGC","outputId":"b65b56fa-c86c-449d-8c97-83e479f537fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"predicted\"] = torch.round(testpreds).numpy()\n\n\n\"\"\"\nthis shows that the model understands the language well \n\n\"\"\"\n\ntest[test.predicted==1].iloc[32:37]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load(\"Model.pt\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model(\"syamasahithi.com/8fh34f3\").squeeze().cpu().detach()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}