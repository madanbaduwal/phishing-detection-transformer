{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42e55fe8",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baa88677",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9MmpNZzbBqkWes0GN8IBQ.png)\n",
    "\n",
    "![](https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/40ffe/transformer.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72604339",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff28b26a",
   "metadata": {},
   "source": [
    "*  Tokenization is the process of encoding a string of text into transformer-readable token ID integers.\n",
    "\n",
    "* Tokenization is cutting input data into meaningful parts that can be embedded into a vector space.\n",
    "  \n",
    "* image is split into patches, text is split into tokens (frequent words) e.g. transformer tokenization\n",
    "\n",
    "* Can add token position is added to their embeddings.\n",
    "Can add tokens for pooling purposes e.g. class token ([CLS]) used for text classification in BERT transformer.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*yM9GYw49kIFf3bKp2Eg2AQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a10f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchtext\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"You can now install TorchText using pip!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a35d6dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "077eaff2",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5cbd6a2",
   "metadata": {},
   "source": [
    "## Normal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bd0e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76cc1cf7",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740a0d38",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*C3a9RL6-SFC6fW8NGpJg5A.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "230d9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim / dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c31d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4150508",
   "metadata": {},
   "source": [
    "# Tokenization vs Embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9efb42",
   "metadata": {},
   "source": [
    "* input is tokenized, the tokens then are embedded\n",
    "* output text embeddings are classified back into tokens, which then can be decoded into text\n",
    "* tokenization converts a text into a list of integers\n",
    "* embedding converts the list of integers into a list of vectors (list of embeddings)\n",
    "* positional information about each token is added to embeddings using positional encodings or embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2ce4550",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc57d58e",
   "metadata": {},
   "source": [
    "## Scale Dot-Product Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b56fea5",
   "metadata": {},
   "source": [
    "\n",
    "![](https://jamesmccaffrey.files.wordpress.com/2020/09/sdpa_picture_and_equation.jpg?w=584&h=246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56570a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import  Tensor\n",
    "import torch.nn.functional as f \n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1,2))\n",
    "    scale = query.size(-1)**0.5\n",
    "    softmax = f.softmax(temp/scale, dim= -1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45f5865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self,dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in,dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in,dim_k)\n",
    "\n",
    "    def forward(self,query: Tensor, key: Tensor, value: Tensor)-> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query),self.k(key),self.v(value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a40ee16f",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d74519",
   "metadata": {},
   "source": [
    "![](https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8aaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in,dim_q,dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "    \n",
    "    def forward(self,query: Tensor,key:Tensor,value: Tensor) -> Tensor:\n",
    "        return self.linear(torch.cat([h(query,key,value) for h in self.heads], dim=-1))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee622138",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "![](https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0f2ec1b",
   "metadata": {},
   "source": [
    "# Residual , Add and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12529120",
   "metadata": {},
   "source": [
    "# Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63991e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9c39143",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "![](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94ae581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e6a537",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![](https://i.stack.imgur.com/nV7Ee.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27657e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424484c8",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1da5c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44df4285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)\n",
    "# torch.Size([64, 16, 512])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92295314",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d9c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Downloading torchdata-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/madan/anaconda3/lib/python3.9/site-packages (from torchdata) (1.26.13)\n",
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /home/madan/anaconda3/lib/python3.9/site-packages (from torchdata) (2.28.1)\n",
      "Collecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 9.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in /home/madan/.local/lib/python3.9/site-packages (from torch==2.0.0->torchdata) (3.1.1)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 KB 9.7 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in /home/madan/.local/lib/python3.9/site-packages (from torch==2.0.0->torchdata) (4.1.1)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /home/madan/.local/lib/python3.9/site-packages (from torch==2.0.0->torchdata) (3.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in /home/madan/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchdata) (1.11.1)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: networkx in /home/madan/.local/lib/python3.9/site-packages (from torch==2.0.0->torchdata) (2.7.1)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 9.7 MB/s eta 0:00:00\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in /home/madan/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/madan/.local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata) (61.2.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/24.0 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145.0/145.0 KB 12.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/madan/anaconda3/lib/python3.9/site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/madan/anaconda3/lib/python3.9/site-packages (from requests->torchdata) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/madan/anaconda3/lib/python3.9/site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/madan/.local/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchdata) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/madan/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchdata) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py): started\n",
      "  Building wheel for lit (setup.py): finished with status 'done'\n",
      "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93600 sha256=97b489bcb5aced7b22889e867b7c949248ef6e52ea731676707dabd58f51c181\n",
      "  Stored in directory: /home/madan/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0 requires torch==1.11.0, but you have torch 2.0.0 which is incompatible.\n",
      "torchtext 0.9.0 requires torch==1.8.0, but you have torch 2.0.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed cmake-3.26.0 lit-16.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 triton-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/madan/anaconda3/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install torchdata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "769a9795",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "https://blog.floydhub.com/the-transformer-in-pytorch/\n",
    "\n",
    "https://github.com/hyunwoongko/transformer\n",
    "\n",
    "https://mohitkpandey.github.io/posts/2020/11/trfm-code/\n",
    "\n",
    "https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea45ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7475a5b0c8a0188b482b5b242b75766a551c9413606b66cf184f5100cbabfaf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
