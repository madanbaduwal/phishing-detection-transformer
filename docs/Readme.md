
Here is the [paper link](./../reading_papers/1706.03762.pdf).

Here is the drive [link of slide](https://docs.google.com/presentation/d/1dyd1HcWbNlfrtDTVjJurKtxxjG8cjE60cNeNa-0_6nU/edit#slide=id.g20e8df68100_0_0).


Slide 1-4 : Madan
Slide 5-7: Prakash


Slide 1: Introduction

Briefly introduce the topic of the paper and why it's important in the field of natural language processing.


Slide 2: Background

Discuss the limitations of traditional recurrent neural networks (RNNs) in processing sequential data.
Explain how the attention mechanism was introduced to overcome these limitations.
Slide 3: The Transformer Architecture

Describe the architecture of the Transformer model, including the self-attention mechanism and feed-forward network.
Explain how this architecture enables parallel processing of input sequences.
Slide 4: Attention Mechanism

Detail the mathematical implementation of the attention mechanism, including the dot-product attention and multi-head attention.
Explain how the attention mechanism allows the model to weigh different parts of the input sequence to generate a highly-contextualized representation.
Slide 5: Experiments

Discuss the experiments conducted in the paper to evaluate the Transformer architecture on various natural language processing tasks.
Summarize the results, showing the superiority of the Transformer over traditional RNNs and convolutional neural networks (CNNs).
Slide 6: Conclusion

Summarize the key contributions of the paper, including the introduction of the Transformer architecture and its application to various NLP tasks.
Discuss the impact of the paper on the field of NLP and future directions for research.
Slide 7: References

Include a list of references cited in the paper.